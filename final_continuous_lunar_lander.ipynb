{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTgqHJuXTQ9F"
      },
      "source": [
        "# Soft Actor-Critic (SAC) â€” Lunar Lander (Continuous)\n",
        "\n",
        "A concise notebook implementing SAC on `LunarLander-v3` (continuous action space)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmM8JPbXTQ9J"
      },
      "source": [
        "## 1. Imports and Device Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvRODokjDOa1"
      },
      "outputs": [],
      "source": [
        "from collections import deque, namedtuple\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTalhrfHDOa2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpaKyCRfTZCT"
      },
      "outputs": [],
      "source": [
        "!pip install swig\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dN5c9tADOa3"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldI6emDvp0zS"
      },
      "outputs": [],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnbxgjHYDOa3"
      },
      "outputs": [],
      "source": [
        "seed = 0\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFE0Pf5aD5wM"
      },
      "source": [
        "## 2. Environment Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcBmI_C3DOa4"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v3\", continuous=True, render_mode=\"rgb_array\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIQuqUzQDOa4"
      },
      "outputs": [],
      "source": [
        "env.reset()\n",
        "frame = env.render()\n",
        "plt.imshow(frame)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tv8Y2OXJDOa4"
      },
      "outputs": [],
      "source": [
        "state_size = env.observation_space.shape\n",
        "num_actions = env.action_space.shape[0]\n",
        "\n",
        "print('State Shape:', state_size)\n",
        "print('Number of actions:', num_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc_MCz0sTQ9U"
      },
      "source": [
        "## 3. Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wX1FA2SBTQ9U"
      },
      "outputs": [],
      "source": [
        "LR = 3e-4\n",
        "# Learning rate\n",
        "\n",
        "gamma = 0.99\n",
        "# Discount factor\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "tau = 0.005\n",
        "# Soft update parameter\n",
        "\n",
        "start_steps = int(1e4)\n",
        "# Number of initial environment steps using random actions\n",
        "# Used to fill replay buffer with diverse experience\n",
        "\n",
        "memory_size = int(1e6)\n",
        "\n",
        "replay_fill = int(1e4)\n",
        "# Minimum number of transitions in replay buffer before training starts\n",
        "\n",
        "target_entropy = - num_actions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPDz2RL7TQ9V"
      },
      "source": [
        "## 4. Actor and Critic Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qY45GOCcDOa5"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(state_size[0]+num_actions, 256)\n",
        "        self.l2 = nn.Linear(256, 256)\n",
        "        self.l3 = nn.Linear(256, 1)\n",
        "    def forward(self, x, a):\n",
        "        y1 = torch.relu(self.l1(torch.cat((x,a),1)))\n",
        "        y2 = torch.relu(self.l2(y1))\n",
        "        y = self.l3(y2)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCzbBR5iDOa5"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(state_size[0], 256)\n",
        "        self.l2 = nn.Linear(256, 256)\n",
        "        self.mu = nn.Linear(256, num_actions)\n",
        "        self.log_sigma = nn.Linear(256, num_actions)\n",
        "    def forward(self, x, deterministic, with_log):\n",
        "        y1 = torch.relu(self.l1(x))\n",
        "        y2 = torch.relu(self.l2(y1))\n",
        "        mu = self.mu(y2)\n",
        "\n",
        "        if deterministic:\n",
        "            action = torch.tanh(mu)\n",
        "            log_prob = None\n",
        "        else:\n",
        "            log_sigma = self.log_sigma(y2)\n",
        "            log_sigma = torch.clamp(log_sigma, min=-10.0, max=2.0)\n",
        "            sigma = torch.exp(log_sigma)\n",
        "            dist = torch.distributions.Normal(mu, sigma)\n",
        "            x_t = dist.rsample()\n",
        "            if with_log:\n",
        "                log_prob = dist.log_prob(x_t).sum(dim=-1, keepdim=True)\n",
        "                log_prob -= (2*(np.log(2) - x_t - nn.functional.softplus(-2*x_t))).sum(dim=-1, keepdim=True)\n",
        "            else:\n",
        "                log_prob = None\n",
        "            action = torch.tanh(x_t)\n",
        "\n",
        "        return action, log_prob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jsQxt8dDOa5"
      },
      "outputs": [],
      "source": [
        "critic_1 = Critic().to(device)\n",
        "critic_2 = Critic().to(device)\n",
        "actor = Actor().to(device)\n",
        "critic_target_1 = Critic().to(device)\n",
        "critic_target_2 = Critic().to(device)\n",
        "log_alpha = torch.tensor(np.log(0.2), dtype=torch.float32, device=device, requires_grad=True)\n",
        "\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=LR)\n",
        "critic_optimizer_1 = optim.Adam(critic_1.parameters(), lr=LR)\n",
        "critic_optimizer_2 = optim.Adam(critic_2.parameters(), lr=LR)\n",
        "log_alpha_optimizer = torch.optim.Adam([log_alpha], lr=LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbY_Vf3ODOa6"
      },
      "outputs": [],
      "source": [
        "critic_target_1.load_state_dict(critic_1.state_dict())\n",
        "critic_target_2.load_state_dict(critic_2.state_dict())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayfD1Ll3DOa6"
      },
      "outputs": [],
      "source": [
        "for param in critic_target_1.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in critic_target_2.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD3IVH2_TQ9Z"
      },
      "source": [
        "## 5. Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtdK3iouDOa6"
      },
      "outputs": [],
      "source": [
        "Experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "memory_buffer = deque(maxlen=memory_size)\n",
        "\n",
        "def get_experiences(memory_buffer, batch_size):\n",
        "\n",
        "    batch = random.sample(memory_buffer, batch_size)\n",
        "    states_list = [e.state for e in batch]\n",
        "    states = torch.tensor(np.vstack(states_list), dtype=torch.float32, device=device)\n",
        "\n",
        "    actions_list = [e.action for e in batch]\n",
        "    actions = torch.tensor(np.vstack(actions_list), dtype=torch.float32, device=device)\n",
        "\n",
        "    rewards_list = [e.reward for e in batch]\n",
        "    rewards = torch.tensor(np.vstack(rewards_list), dtype=torch.float32, device=device)\n",
        "\n",
        "    next_states_list = [e.next_state for e in batch]\n",
        "    next_states = torch.tensor(np.vstack(next_states_list), dtype=torch.float32, device=device)\n",
        "\n",
        "    dones_list = [e.done for e in batch]\n",
        "    dones = torch.tensor(np.vstack(dones_list), dtype=torch.float32, device=device)\n",
        "\n",
        "    return (states, actions, rewards, next_states, dones)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV513YCGTQ9a"
      },
      "source": [
        "## 6. SAC Update Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4y4WhTUpDOa6"
      },
      "outputs": [],
      "source": [
        "def update_networks(experiences, critic_1, critic_2, critic_target_1, critic_target_2, actor, gamma, log_alpha, criterion, critic_optimizer_1, critic_optimizer_2, actor_optimizer, log_alpha_optimizer, target_entropy):\n",
        "\n",
        "    states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "    q_vals_1 = critic_1(states, actions)\n",
        "    q_vals_2 = critic_2(states, actions)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        next_actions, log_probs = actor(next_states, False, True)\n",
        "        log_probs = log_probs\n",
        "        next_q_value_1 = critic_target_1(next_states, next_actions)\n",
        "        next_q_value_2 = critic_target_2(next_states, next_actions)\n",
        "        next_q_value = torch.min(next_q_value_1, next_q_value_2)\n",
        "        target = rewards + gamma * (1 - dones) * (next_q_value - torch.exp(log_alpha)*log_probs)\n",
        "\n",
        "    loss_1 = criterion(q_vals_1, target)\n",
        "    loss_2 = criterion(q_vals_2, target)\n",
        "\n",
        "    critic_optimizer_1.zero_grad()\n",
        "    loss_1.backward()\n",
        "    critic_optimizer_1.step()\n",
        "\n",
        "    critic_optimizer_2.zero_grad()\n",
        "    loss_2.backward()\n",
        "    critic_optimizer_2.step()\n",
        "\n",
        "    for param_1, param_2 in zip(critic_1.parameters(), critic_2.parameters()):\n",
        "        param_1.requires_grad = False\n",
        "        param_2.requires_grad = False\n",
        "\n",
        "    actions_pi, log_probs_pi = actor(states, False, True)\n",
        "    log_probs_pi = log_probs_pi\n",
        "    q_value_pi_1 = critic_1(states, actions_pi)\n",
        "    q_value_pi_2 = critic_2(states, actions_pi)\n",
        "    q_value_pi = torch.min(q_value_pi_1, q_value_pi_2)\n",
        "\n",
        "    actor_loss = - torch.mean(q_value_pi - torch.exp(log_alpha).detach() * log_probs_pi)\n",
        "    actor_optimizer.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    actor_optimizer.step()\n",
        "\n",
        "    alpha_loss = (torch.exp(log_alpha) * (-log_probs_pi - target_entropy).detach()).mean()\n",
        "    log_alpha_optimizer.zero_grad()\n",
        "    alpha_loss.backward()\n",
        "    log_alpha_optimizer.step()\n",
        "\n",
        "    for param_1, param_2 in zip(critic_1.parameters(), critic_2.parameters()):\n",
        "        param_1.requires_grad = True\n",
        "        param_2.requires_grad = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yu22brblDOa6"
      },
      "outputs": [],
      "source": [
        "def soft_update_target_network(critic_1, critic_2, critic_target_1, critic_target_2, tau):\n",
        "\n",
        "    for target_param, param in zip(critic_target_1.parameters(), critic_1.parameters()):\n",
        "\n",
        "        target_param.data = (tau * param.data + (1-tau) * target_param.data )\n",
        "\n",
        "    for target_param, param in zip(critic_target_2.parameters(), critic_2.parameters()):\n",
        "\n",
        "        target_param.data = (tau * param.data + (1-tau) * target_param.data )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfx8CamJTQ9c"
      },
      "source": [
        "## 7. Training Loop\n",
        "\n",
        "Main training loop: environment interaction, fill replay buffer, sample batches, update actor/critics, soft-update target networks, and periodic logging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NAkVrkQDOa6"
      },
      "outputs": [],
      "source": [
        "num_episodes = 2000\n",
        "start_episode = 0\n",
        "criterion = nn.MSELoss()\n",
        "num_to_print = 100\n",
        "total_rewards_list = []\n",
        "max_steps = 1000\n",
        "CHECKPOINT_PATH = \"sac_lunar_lander_checkpoint.pt\"\n",
        "\n",
        "for i in range(start_episode, num_episodes):\n",
        "    state = env.reset()[0]\n",
        "    total_rewards = 0\n",
        "\n",
        "    for j in range(max_steps):\n",
        "        with torch.no_grad():\n",
        "            action, _ = actor(torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0), False, False)\n",
        "            action = action.cpu().numpy()[0]\n",
        "\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        total_rewards += reward\n",
        "\n",
        "        exper = Experience(state, action, reward, next_state, done)\n",
        "        memory_buffer.append(exper)\n",
        "\n",
        "        if len(memory_buffer) >= replay_fill:\n",
        "\n",
        "            experiences = get_experiences(memory_buffer, batch_size)\n",
        "\n",
        "            update_networks(experiences, critic_1, critic_2, critic_target_1, critic_target_2,\n",
        "                             actor, gamma, log_alpha, criterion, critic_optimizer_1, critic_optimizer_2,\n",
        "                               actor_optimizer, log_alpha_optimizer, target_entropy)\n",
        "\n",
        "            soft_update_target_network(critic_1, critic_2, critic_target_1, critic_target_2, tau)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    total_rewards_list.append(total_rewards)\n",
        "    avg_last_rewards = np.mean(total_rewards_list[-num_to_print:])\n",
        "\n",
        "    if (i+1) % 20 == 0:\n",
        "        checkpoint = {\n",
        "            \"episode\": i + 1,\n",
        "            \"actor\": actor.state_dict(),\n",
        "            \"critic_1\": critic_1.state_dict(),\n",
        "            \"critic_2\": critic_2.state_dict(),\n",
        "            \"critic_target_1\": critic_target_1.state_dict(),\n",
        "            \"critic_target_2\": critic_target_2.state_dict(),\n",
        "            \"actor_optimizer\": actor_optimizer.state_dict(),\n",
        "            \"critic_optimizer_1\": critic_optimizer_1.state_dict(),\n",
        "            \"critic_optimizer_2\": critic_optimizer_2.state_dict(),\n",
        "            \"log_alpha\": log_alpha.detach().cpu(),\n",
        "            \"log_alpha_optimizer\": log_alpha_optimizer.state_dict(),\n",
        "            \"memory_buffer\": list(memory_buffer),\n",
        "            \"total_rewards_list\": total_rewards_list,\n",
        "        }\n",
        "\n",
        "        torch.save(checkpoint, CHECKPOINT_PATH)\n",
        "        print(f\"\\rEpisode {i+1} | {avg_last_rewards} \", end=\"\")\n",
        "\n",
        "    if avg_last_rewards >= 200.0:\n",
        "        print(f\"\\n\\nEnvironment solved in {i+1} episodes!\")\n",
        "        torch.save(actor.state_dict(), \"sac_lunar_lander_actor_network.pth\")\n",
        "        break\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYO9Z_owTQ9e"
      },
      "source": [
        "## 8. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOBc6_AFTQ9e"
      },
      "outputs": [],
      "source": [
        "rewards_list = []\n",
        "episodes = 50\n",
        "\n",
        "actor.eval()\n",
        "with torch.no_grad():\n",
        "    for ep in range(episodes):\n",
        "        state = env.reset()[0]\n",
        "        done = False\n",
        "        total_reward = 0.0\n",
        "\n",
        "        while not done:\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "            action, _ = actor(state_tensor, True, False)\n",
        "            action = action.cpu().numpy()[0]\n",
        "\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        rewards_list.append(total_reward)\n",
        "\n",
        "print(f\"Average reward over {episodes} episodes:\", np.mean(rewards_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbVkf2aaTQ9f"
      },
      "source": [
        "## 9. Video Recording"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyZGbh-aT-Tf"
      },
      "outputs": [],
      "source": [
        "import imageio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IleVvmAQBXOA"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def create_video(filename, env, actor, fps=30, max_steps=1000):\n",
        "    frames = []\n",
        "\n",
        "    device = next(actor.parameters()).device\n",
        "    state = env.reset()[0]\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.tensor(\n",
        "                state,\n",
        "                dtype=torch.float32,\n",
        "                device=device\n",
        "            ).unsqueeze(0)\n",
        "\n",
        "            action, _ = actor(state_tensor, True, False)\n",
        "\n",
        "        action = action.cpu().numpy()[0]\n",
        "\n",
        "        state, _, done, _, _ = env.step(action)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    imageio.mimsave(\n",
        "        filename,\n",
        "        frames,\n",
        "        fps=fps,\n",
        "        codec=\"libx264\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aY2d9TaBZGP"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "import base64\n",
        "def embed_mp4(filename):\n",
        "    video = open(filename, \"rb\").read()\n",
        "    b64 = base64.b64encode(video)\n",
        "    tag = \"\"\"\n",
        "    <video width=\"840\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "    Your browser does not support the video tag.\n",
        "    </video>\"\"\".format(\n",
        "        b64.decode()\n",
        "    )\n",
        "\n",
        "    return IPython.display.HTML(tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVvzmy37Bd_F"
      },
      "outputs": [],
      "source": [
        "filename = \"./lunar_lander.mp4\"\n",
        "create_video(filename, env, actor)\n",
        "embed_mp4(filename)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
